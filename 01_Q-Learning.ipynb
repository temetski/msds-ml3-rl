{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c734089d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Q-Learning\n",
    "\n",
    "*Prepared by Damian Dailisan*\n",
    "\n",
    "---\n",
    "\n",
    "## Problem: `Taxi-v3`\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "[Dietterich2000] T Erez, Y Tassa, E Todorov, \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\", 2011.\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/envs/Taxi-v3/) provides a convenient environment for this scenario for us to focus on the actual problem at hand.\n",
    "It even has a way to render a representation of the problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4f37e7-cf7f-40e0-b1b3-fd41cdba808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T04:30:22.754312Z",
     "start_time": "2022-01-31T04:30:22.626760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81695047",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "Taxi states $\\mathbf{s}$ for this environment involve four variables:\n",
    "* taxi row location (0,1,2,3,4)\n",
    "* taxi column location (0,1,2,3,4)\n",
    "* passenger position (R,G,Y,B,in_taxi)\n",
    "* destination index (R,G,Y,B)\n",
    "\n",
    "All in all, there are a total of $5\\times 5\\times 5\\times 4=500$ possible states in the entire system.\n",
    "As a convenience, the environment provides an [encoding](https://github.com/openai/gym/blob/91d278f2dd7fa4493ad12ad5cba9018726415f3c/gym/envs/toy_text/taxi.py#L184) which converts each possible combination of state variables $s_i$ into a single number from 0-499."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a875974-9327-4b95-9665-655004063bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aee255",
   "metadata": {},
   "source": [
    "### Actions and Rewards\n",
    "\n",
    "In reinforcement learning, it is important to also define the actions $a$ that can be taken from the current state.\n",
    "To facilitate learning, each corresponding state-action pair must also have a corresponding reward $r$ which provides a heuristic feedback measure on how well the agent is achieving is task.\n",
    "\n",
    "An agent in `taxi-v3` can perform 6 actions: \n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "And there are three types of rewards possible:\n",
    "- -1 per step reward unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10  executing \"pickup\" and \"drop-off\" actions illegally.\n",
    "\n",
    "\n",
    "### Transitions\n",
    "`taxi-v3` also maintains a lookup table of states, actions, and the corresponding transitions and rewards in `env.P`.\n",
    "This transition table provides the probability of state $s'$ from ($s$,$a$), along with the corresponding probability of each outcome, rewards, and boolean variable indicating if the environment should terminate.\n",
    "Particularly for `taxi-v3`, there is only a single possible outcome state $s'$ for each state-action pair.\n",
    "In this sense, actions in `taxi-v3` are *deterministic*.\n",
    "Other environments and actions might call for *stochastic* actions; where a state-action pair may result to multiple possible states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb98ffc6-cdc0-4d3f-9bc2-9af3a20fded1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800c3ba",
   "metadata": {},
   "source": [
    "## Solving `Taxi-v3` without RL\n",
    "\n",
    "Let us *solve* the `Taxi-v3` environment without using RL.\n",
    "To do this, we can use the default transition matrix defined by the environment.\n",
    "We can sample actions given the current state (note: actions are equally likely) and perform the chosen action to transition to the next state and incurring the corresponding rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96de57b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3239\n",
      "Penalties incurred: 1061\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "steps = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    steps += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(steps))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441e7d0",
   "metadata": {},
   "source": [
    "We can see that the agent takes a long time to accomplish its task (transporting a passenger from the origin to destination).\n",
    "Along the way, the agent also incurs a lot of penalties for attempting to pick up a non-existing passenger, or dropping off the passenger at the wrong location.\n",
    "\n",
    "Even if we re-run this several times, we are likely to get similarly bad results because there are no updates to the **probabilities of actions** --- the agent is not learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f7639",
   "metadata": {},
   "source": [
    "### Animation\n",
    "\n",
    "This is an animation of our naive agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8966f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 3239\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, dt=0.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(dt)\n",
    "        \n",
    "print_frames(frames, dt=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3900bc7",
   "metadata": {},
   "source": [
    "## Q learning\n",
    "\n",
    "To make our agent learn from its past experiences in the environment, we need a way for the agent to quantitatively compare different action outcomes.\n",
    "One such approach is to define an *state-action value function*, which we denote $Q(s,a)$ (Q here can be thought of as *quality*).\n",
    "$$ Q(s,a)= \\mathbb{E}(r + \\gamma \\max_{a} Q(s',a)) $$\n",
    "The $Q$-value estimates the expected cumulative reward given the current state and action.\n",
    "The above equation is also known as a Bellman equation.\n",
    "The Bellman equation tells us that the maximum future reward is the reward the agent received for entering the current state $s$ plus the maximum future reward for the next state $s'$.\n",
    "\n",
    "Now that the agent has a way to quantify the consequences of its actions, we can define the driving force of the agent: given its current state, it wants to pick the action that provides the maximum $Q$-value.\n",
    "However, as with all machine learning, it is also important for an agent to once in a while explore other actions.\n",
    "Typically, this is implemented using an $\\epsilon$-greedy (epsilon-greedy) approach: with a probability $\\epsilon$, the agent will take a random action.\n",
    "\n",
    "We can then let the agent interact repeatedly with the environment to experience the different states and rewards possible.\n",
    "While interactive with the environment, the agent then *learns* by updating the table of $Q$-values for all possible states using the following rule:\n",
    "$$ Q_{t+1}(s_t,a_t)=(1-\\alpha)Q_t(s_t,a_t) + \\alpha (r_t + \\gamma \\max_a Q(s_{t+1},a))$$\n",
    "where $\\alpha$ is a learning rate, and $\\gamma$ is the discount factor.\n",
    "The discount factor provides a weighting that places importance on rewards obtained sooner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d889791",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let us implement training for the agent.\n",
    "Some preliminaries: we denote a single session of the environment as an **episode**.\n",
    "**Episodes** can be thought of a intantiations of a RL session (think of video games) that an agent \"plays through\" until it succeeds or fails to reach its objective.\n",
    "\n",
    "We can start with a `q_table` with values of 0 for all state-action pairs: this is a naive agent with no idea of the rules and dynamics of this world.\n",
    "The agent then starts interacting with the environment, choosing actions based on our $\\epsilon$-greedy approach.\n",
    "With each action the agent takes, it receives a reward which it can then use to *learn* more about the environment by updating the $Q$ table.\n",
    "Over time, the agent should learn to pick successive actions that allow it to solve the `taxi-v3` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a089ba-3b6c-4e46-adea-58cfdeac42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000: 14.55 timesteps/episode\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "num_episodes = 100000\n",
    "\n",
    "# For plotting metrics\n",
    "all_steps = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(num_episodes): # run for several episodes\n",
    "    state = env.reset() # start with a random state\n",
    "\n",
    "    steps, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    all_steps.append(steps)\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i+1}: {np.mean(all_steps)} timesteps/episode\")\n",
    "        all_steps = []\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153acce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(~np.all(q_table, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2c026",
   "metadata": {},
   "source": [
    "### Evaluate agent's performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e0395f-32d1-4f37-a9b6-b87527750e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.8\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_steps, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset() # start with a random state\n",
    "    steps, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_steps += steps\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_steps / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c18125",
   "metadata": {},
   "source": [
    "### Animation\n",
    "\n",
    "Here is an animation of a trained agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8691d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 12\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "steps = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames_q = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames_q.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    steps += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(steps))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad81d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 12\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames_q, dt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c1d22-7359-4fce-bdbe-1ed1c3387966",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "2. https://gym.openai.com/envs/Taxi-v3/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
