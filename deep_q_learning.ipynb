{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reinforcement Learning (DQN) Tutorial\n",
    "\n",
    "*Prepared by Damian Dailisan*\n",
    "\n",
    "---\n",
    "\n",
    "## Problem: `LunarLander-v2`\n",
    "\n",
    "This example shows an implementation of a Deep Q Learning (DQN) agent\n",
    "trained to solve the `LunarLander-v2` task from the [OpenAI Gym](https://gym.openai.com/envs/LunarLander-v2/).\n",
    "\n",
    "<video controls autoplay=true src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/LunarLander-v2/original.mp4\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "This environment is a classic rocket trajectory optimization problem.\n",
    "The goal is to train an agent to control the landing of a rocket into a landing pad.\n",
    "In this environment, landing outside the landing pad is possible.\n",
    "Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "\n",
    "### Actions\n",
    "The agent has to decide between four actions --- do nothing, fire left orientation engine, fire main engine, fire right orientation engine --- with the objective of landing on the landing pad.\n",
    "\n",
    "### States\n",
    "The state of the lander is encoded in 8 variables:\n",
    "- x position\n",
    "- y position\n",
    "- x velocity\n",
    "- y velocity\n",
    "- angle\n",
    "- angular velocity\n",
    "- left leg touching ground\n",
    "- right leg touching ground\n",
    "\n",
    "### Rewards\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action.\n",
    "This environment rewards the agent for the following:\n",
    "- -100 lander crashed or lands outside landing pad (ends an episode)\n",
    "- +100 lander comes to rest within landing pad (ends an episode)\n",
    "- +10 for each leg currently on the ground (lifting a leg incurs a -10 reward)\n",
    "- -0.3 for each frame the main engine is used\n",
    "- -0.03 for using the side engines\n",
    "- There are miscellaneous positive (negative) rewards for decreasing (increasing) the distance to the landing pads.\n",
    "\n",
    "The rewards incentivise the agent for landing inside the landing pad on both legs, while using the least amount of fuel as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyglet\n",
    "# !pip install \"gym[Box_2D]\"\n",
    "# !pip install tensorflow\n",
    "# !pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddailisan/.conda-envs/deeprl/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # this is a CPU-bound process\n",
    "seed = 42\n",
    "\n",
    "# load the environment from openai gym\n",
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.seed(seed)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Q-network\n",
    "\n",
    "Our model will be a fully connected neural network with two [64,64] hidden layers that takes in state observations $s$as input.\n",
    "It has four outputs, representing $Q(s, \\mathrm{do nothing})$, \n",
    "$Q(s, \\mathrm{fire left})$, $Q(s, \\mathrm{fire main})$, and $Q(s, \\mathrm{fire right})$. \n",
    "In effect, the network is trying to predict the *expected return* of taking each action given the current input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n # this should be 4\n",
    "num_observations =  len(state) # this is 8\n",
    "\n",
    "def create_q_model():\n",
    "    inputs = layers.Input(shape=(num_observations))\n",
    "\n",
    "    layer1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Dense(64, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Dense(64, activation=\"relu\")(layer2)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=None)(layer3)\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay is a useful trick used in DQNs, particularly when subsequent states are highly correlated to each other.\n",
    "Instead of batching consecutive experiences together and using this to train the DQN, we can instead temporarily store the recent experiences of the agent in a buffer.\n",
    "This allows us to reuse this data later.\n",
    "Random samples from the replay buffer results in a batch of transitions that are decorrelated.\n",
    "It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "The replay buffer is a first-in-first-out (FIFO) storage with finite capacity, which we will implement as a `deque`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append((*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN algorithm\n",
    "\n",
    "\n",
    "Our aim is to train a policy that maximizes the discounted,\n",
    "cumulative reward\n",
    "$R = \\sum_{t=t_0}^{\\tau} \\gamma^{t} r_t$, where\n",
    "$R$ is also known as the *return*. The discount,\n",
    "$\\gamma$, is a constant between $0$ and $1$\n",
    "that ensures the sum converges.\n",
    "The discount is a weight that makes rewards from the uncertain far\n",
    "future less important than the ones in the near future.\n",
    "\n",
    "$Q$-learning tries to find the function\n",
    "$Q(s,a)$ that rstimates our return, if we were to take an action in a given\n",
    "state.\n",
    "This allows us to construct a policy $\\pi$ that maximizes our\n",
    "rewards:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\!\\max_a \\ Q(s, a) $$\n",
    "\n",
    "The challenge here is to find $Q$ that suitably defines our environment.\n",
    "Because neural networks are universal function\n",
    "approximators, one approach is to train a neural network to resemble $Q$.\n",
    "This offers a vast improvement over the tabular approach, which can get numerically intractable once there are a lot more states and actions to consider, as is in a more complex environment.\n",
    "\n",
    "We can use the Bellman Equation:\n",
    "$$ Q(s,a)= \\mathbb{E}(r + \\gamma \\max_{a} Q(s',a)) $$\n",
    "to define a loss function for our problem.\n",
    "Here, we use the temporal difference error, $\\delta$:\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "as the loss function.\n",
    "In addition to this error, we use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss) to train the neural network.\n",
    "For small errors, the Huber loss behaves similar to the mean squared error, while for large errors it is similar to the mean absolute error.\n",
    "The Huber loss is more robust to outliers due to noisy estimates of $Q$.\n",
    "The network is trained over a batch of transitions $B$ sampled from the replay memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "   \n",
    "For convenience and numerical stability reasons, we also make use of two neural networks: the policy and target networks.\n",
    "The policy network represents the first $Q$ term in the temporal difference error, while the target network is the second $Q$ term.\n",
    "The target network copies its weights from the policy network over a longer interval.\n",
    "Avoiding frequent updates to the target network ensures the stability of training the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-06 10:16:07.946861: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-02-06 10:16:07.946923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-02-06 10:16:07.947261: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model_policy = create_q_model()\n",
    "\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every `update_target_network` steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Some hyperparameters:\n",
    "\n",
    "-  `epsilon_max`, `epsilon_min`, and `exploration_fraction` control the annealed value of epsilon over training steps.\n",
    "   This allows us to decay the emount of exploration of the agent over time.\n",
    "-  `update_target_network` sets the interval on how often the target network is updated.\n",
    "-  `train_freq` is the number of actions before the policy network weights are updated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fd7e04118c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fd7e04118c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "running reward: -143.24 at episode 56, frames: 5000\n",
      "running reward: -158.58 at episode 113, frames: 10000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "running reward: -126.60 at episode 170, frames: 15000\n",
      "running reward: -175.56 at episode 226, frames: 20000\n",
      "running reward: -152.75 at episode 284, frames: 25000\n",
      "running reward: -141.83 at episode 338, frames: 30000\n",
      "running reward: -136.92 at episode 396, frames: 35000\n",
      "running reward: -132.38 at episode 456, frames: 40000\n",
      "running reward: -127.21 at episode 514, frames: 45000\n",
      "running reward: -132.22 at episode 568, frames: 50000\n",
      "running reward: -87.94 at episode 623, frames: 55000\n",
      "running reward: -100.13 at episode 676, frames: 60000\n",
      "running reward: -98.04 at episode 728, frames: 65000\n",
      "running reward: -82.85 at episode 781, frames: 70000\n",
      "running reward: -103.03 at episode 832, frames: 75000\n",
      "running reward: -94.44 at episode 885, frames: 80000\n",
      "running reward: -69.76 at episode 937, frames: 85000\n",
      "running reward: -95.45 at episode 990, frames: 90000\n",
      "running reward: -83.18 at episode 1041, frames: 95000\n",
      "running reward: -84.95 at episode 1094, frames: 100000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "running reward: -85.54 at episode 1142, frames: 105000\n",
      "running reward: -56.32 at episode 1191, frames: 110000\n",
      "running reward: -74.26 at episode 1240, frames: 115000\n",
      "running reward: -61.98 at episode 1291, frames: 120000\n",
      "running reward: -80.85 at episode 1344, frames: 125000\n",
      "running reward: -53.14 at episode 1389, frames: 130000\n",
      "running reward: -59.93 at episode 1434, frames: 135000\n",
      "running reward: -32.42 at episode 1480, frames: 140000\n",
      "running reward: -54.98 at episode 1530, frames: 145000\n",
      "running reward: -49.33 at episode 1581, frames: 150000\n",
      "running reward: -41.61 at episode 1629, frames: 155000\n",
      "running reward: -38.80 at episode 1679, frames: 160000\n",
      "running reward: -34.68 at episode 1726, frames: 165000\n",
      "running reward: -45.01 at episode 1774, frames: 170000\n",
      "running reward: -38.23 at episode 1825, frames: 175000\n",
      "running reward: -36.95 at episode 1868, frames: 180000\n",
      "running reward: -31.05 at episode 1918, frames: 185000\n",
      "running reward: -27.74 at episode 1966, frames: 190000\n",
      "running reward: -28.06 at episode 2015, frames: 195000\n",
      "running reward: -25.44 at episode 2059, frames: 200000\n",
      "running reward: -34.13 at episode 2098, frames: 205000\n",
      "running reward: -28.89 at episode 2137, frames: 210000\n",
      "running reward: -17.01 at episode 2182, frames: 215000\n",
      "running reward: -26.19 at episode 2215, frames: 220000\n",
      "running reward: -14.13 at episode 2255, frames: 225000\n",
      "running reward: 1.31 at episode 2290, frames: 230000\n",
      "running reward: -24.06 at episode 2325, frames: 235000\n",
      "running reward: -3.31 at episode 2360, frames: 240000\n",
      "running reward: 18.59 at episode 2392, frames: 245000\n",
      "running reward: 26.81 at episode 2417, frames: 250000\n",
      "running reward: 0.44 at episode 2457, frames: 255000\n",
      "running reward: 8.41 at episode 2492, frames: 260000\n",
      "running reward: -28.29 at episode 2516, frames: 265000\n",
      "running reward: 9.97 at episode 2541, frames: 270000\n",
      "running reward: 33.10 at episode 2561, frames: 275000\n",
      "running reward: 16.93 at episode 2595, frames: 280000\n",
      "running reward: 6.49 at episode 2616, frames: 285000\n",
      "running reward: -2.55 at episode 2639, frames: 290000\n",
      "running reward: 49.36 at episode 2660, frames: 295000\n",
      "running reward: 47.21 at episode 2679, frames: 300000\n",
      "running reward: 50.90 at episode 2697, frames: 305000\n",
      "running reward: 112.59 at episode 2714, frames: 310000\n",
      "running reward: 82.66 at episode 2732, frames: 315000\n",
      "running reward: 116.83 at episode 2744, frames: 320000\n",
      "running reward: 129.24 at episode 2754, frames: 325000\n",
      "running reward: 71.89 at episode 2762, frames: 330000\n",
      "running reward: 39.07 at episode 2773, frames: 335000\n",
      "running reward: 31.51 at episode 2779, frames: 340000\n",
      "running reward: 50.09 at episode 2792, frames: 345000\n",
      "running reward: 124.47 at episode 2803, frames: 350000\n",
      "running reward: 20.89 at episode 2824, frames: 355000\n",
      "running reward: 76.12 at episode 2838, frames: 360000\n",
      "running reward: 118.10 at episode 2855, frames: 365000\n",
      "running reward: 104.54 at episode 2862, frames: 370000\n",
      "running reward: 97.73 at episode 2892, frames: 375000\n",
      "running reward: 92.05 at episode 2908, frames: 380000\n",
      "running reward: 85.96 at episode 2927, frames: 385000\n",
      "running reward: 126.31 at episode 2937, frames: 390000\n",
      "running reward: 147.60 at episode 2948, frames: 395000\n",
      "running reward: 140.39 at episode 2957, frames: 400000\n",
      "running reward: 108.62 at episode 2966, frames: 405000\n",
      "running reward: 56.31 at episode 2973, frames: 410000\n",
      "running reward: 111.51 at episode 2990, frames: 415000\n",
      "running reward: 158.09 at episode 3005, frames: 420000\n",
      "running reward: 104.36 at episode 3028, frames: 425000\n",
      "running reward: 114.78 at episode 3047, frames: 430000\n",
      "running reward: 156.95 at episode 3062, frames: 435000\n",
      "running reward: 137.14 at episode 3080, frames: 440000\n",
      "running reward: 155.18 at episode 3097, frames: 445000\n",
      "running reward: 178.38 at episode 3110, frames: 450000\n",
      "running reward: 50.99 at episode 3134, frames: 455000\n",
      "running reward: 50.69 at episode 3159, frames: 460000\n",
      "running reward: 63.03 at episode 3165, frames: 465000\n",
      "running reward: 118.42 at episode 3176, frames: 470000\n",
      "running reward: 147.65 at episode 3198, frames: 475000\n",
      "running reward: 125.76 at episode 3214, frames: 480000\n",
      "running reward: 176.22 at episode 3234, frames: 485000\n",
      "running reward: 197.82 at episode 3249, frames: 490000\n",
      "running reward: 200.42 at episode 3262, frames: 495000\n",
      "running reward: 166.68 at episode 3283, frames: 500000\n",
      "running reward: 200.16 at episode 3302, frames: 505000\n",
      "running reward: 186.47 at episode 3323, frames: 510000\n",
      "running reward: 169.44 at episode 3340, frames: 515000\n",
      "running reward: 206.24 at episode 3361, frames: 520000\n",
      "running reward: 104.09 at episode 3385, frames: 525000\n",
      "running reward: 121.28 at episode 3410, frames: 530000\n",
      "running reward: 85.59 at episode 3429, frames: 535000\n",
      "running reward: 108.02 at episode 3450, frames: 540000\n",
      "running reward: 204.75 at episode 3472, frames: 545000\n",
      "running reward: 216.48 at episode 3491, frames: 550000\n",
      "running reward: 165.89 at episode 3512, frames: 555000\n",
      "running reward: 96.15 at episode 3537, frames: 560000\n",
      "running reward: 138.32 at episode 3562, frames: 565000\n",
      "running reward: 130.36 at episode 3579, frames: 570000\n",
      "running reward: 194.54 at episode 3599, frames: 575000\n",
      "running reward: 154.67 at episode 3618, frames: 580000\n",
      "running reward: 180.32 at episode 3639, frames: 585000\n",
      "running reward: 132.86 at episode 3654, frames: 590000\n",
      "running reward: 147.02 at episode 3671, frames: 595000\n",
      "running reward: 150.51 at episode 3682, frames: 600000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon_min = 0.05  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon = epsilon_max  # Epsilon greedy parameter\n",
    "batch_size = 64  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000 # just a safety constraint\n",
    "exploration_fraction = 0.6 # Fraction of frames for exploration\n",
    "buffer_size = 50000 # Maximum replay length\n",
    "train_freq = 4 # Train the model after 4 actions\n",
    "update_target_network = 200 # How often to update the target network\n",
    "\n",
    "# Deepmind paper used RMSProp however then Adam optimizer is faster\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "episode_reward_history = [0.]\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "\n",
    "loss_function = keras.losses.Huber() # Using huber loss for stability\n",
    "\n",
    "# Experience replay buffers\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "num_timesteps = 600000 # longer to train\n",
    "# num_timesteps = 10000 # debug\n",
    "epsilon_greedy_frames = num_timesteps*exploration_fraction\n",
    "\n",
    "state = env.reset()\n",
    "step_count = 0\n",
    "for frame_count in range(1,num_timesteps+1):\n",
    "    # env.render(); Adding this line would show the attempts\n",
    "    # of the agent in a pop up window.\n",
    "\n",
    "    # Use epsilon-greedy for exploration\n",
    "    if epsilon > np.random.rand(1)[0]:\n",
    "        # Take random action\n",
    "        action = np.random.choice(num_actions)\n",
    "    else:\n",
    "        # Predict action Q-values from state\n",
    "        action_probs = model_policy(state[np.newaxis], training=False)\n",
    "        # Take best action\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    # Linear Decay probability of taking random action\n",
    "    epsilon -= (epsilon_max - epsilon_min)/epsilon_greedy_frames\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    # Apply the sampled action in our environment\n",
    "    state_next, reward, done, _ = env.step(action)\n",
    "\n",
    "#     episode_reward += reward\n",
    "    episode_reward_history[-1] += reward\n",
    "    \n",
    "    # Save actions and states in replay buffer\n",
    "    # replay_buffer.append((action, state, state_next, reward, done))\n",
    "    replay_buffer.push((action, state, state_next, reward, done))\n",
    "\n",
    "    state = state_next\n",
    "\n",
    "    # Update every fourth frame and once batch size is over 32\n",
    "    if frame_count % train_freq == 0 and len(replay_buffer) > batch_size:\n",
    "        # sample the replay buffer\n",
    "        samples = replay_buffer.sample(batch_size)\n",
    "        action_sample = [sample[0] for sample in samples]\n",
    "        state_sample = np.array([sample[1] for sample in samples])\n",
    "        state_next_sample = np.array([sample[2] for sample in samples])\n",
    "        rewards_sample = [sample[3] for sample in samples]\n",
    "        done_sample = tf.convert_to_tensor(\n",
    "            [float(sample[4]) for sample in samples]\n",
    "        )\n",
    "\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = model_target.predict(state_next_sample)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)*(1 - done_sample)\n",
    "        # final frame has no future reward\n",
    "        \n",
    "        # # If final frame set the last value to -1\n",
    "        # updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = model_policy(state_sample)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, model_policy.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model_policy.trainable_variables))\n",
    "\n",
    "    if frame_count % update_target_network == 0:\n",
    "        # update the the target network with new weights\n",
    "        model_target.set_weights(model_policy.get_weights())\n",
    "        \n",
    "    # Log details\n",
    "    if frame_count%(5000) == 0:\n",
    "        print(f\"running reward: {running_reward:.2f} at episode {episode_count}, frames: {frame_count}\")\n",
    "\n",
    "    step_count +=1\n",
    "    if step_count==max_steps_per_episode:\n",
    "        # its taking too long, reset\n",
    "        done = True\n",
    "        step_count = 0\n",
    "        \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "            \n",
    "        # Update running reward to check condition for solving\n",
    "        if len(episode_reward_history) > 20:\n",
    "            del episode_reward_history[:1]\n",
    "        running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "        episode_reward_history.append(0)\n",
    "\n",
    "        episode_count += 1\n",
    "    \n",
    "\n",
    "    if frame_count in [1000, 10000, 100000, num_timesteps]:\n",
    "        model_policy.save(f\"dqn_{frame_count}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save this trained model for reuse later (as it takes some time to train the model until it performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 153.58\t Num episodes: 12\n"
     ]
    }
   ],
   "source": [
    "model_policy=keras.models.load_model(f\"dqn_600000.h5\", compile=False)\n",
    "state = env.reset()\n",
    "done = False\n",
    "episode_rewards=[0]\n",
    "steps=0\n",
    "for i in range(5000):  \n",
    "#     env.render() # for visualization, must be done on a local machine\n",
    "\n",
    "    action_probs = model_policy(state[np.newaxis], training=False)\n",
    "    action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    # Apply the sampled action in our environment\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    episode_rewards[-1] += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if steps==max_steps_per_episode:\n",
    "        done = True\n",
    "        steps = 0\n",
    "        \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "\n",
    "# Compute mean reward for the last 100 episodes\n",
    "print(f\"Mean reward: {np.mean(episode_rewards[-20:]):.2f}\\t Num episodes: {len(episode_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "2. https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "3. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading\n",
    "4. https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeprl]",
   "language": "python",
   "name": "conda-env-deeprl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
