{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reinforcement Learning (DQN) Tutorial\n",
    "\n",
    "*Prepared by Damian Dailisan*\n",
    "\n",
    "---\n",
    "\n",
    "## Problem: `LunarLander-v2`\n",
    "\n",
    "This example shows an implementation of a Deep Q Learning (DQN) agent\n",
    "trained to solve the `LunarLander-v2` task from the [OpenAI Gym](https://gym.openai.com/envs/LunarLander-v2/).\n",
    "\n",
    "<video controls autoplay=true src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/LunarLander-v2/original.mp4\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "This environment is a classic rocket trajectory optimization problem.\n",
    "The goal is to train an agent to control the landing of a rocket into a landing pad.\n",
    "In this environment, landing outside the landing pad is possible.\n",
    "Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "\n",
    "### Actions\n",
    "The agent has to decide between four actions --- do nothing, fire left orientation engine, fire main engine, fire right orientation engine --- with the objective of landing on the landing pad.\n",
    "\n",
    "### States\n",
    "The state of the lander is encoded in 8 variables:\n",
    "- x position\n",
    "- y position\n",
    "- x velocity\n",
    "- y velocity\n",
    "- angle\n",
    "- angular velocity\n",
    "- left leg touching ground\n",
    "- right leg touching ground\n",
    "\n",
    "### Rewards\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action.\n",
    "This environment rewards the agent for the following:\n",
    "- -100 lander crashed or lands outside landing pad (ends an episode)\n",
    "- +100 lander comes to rest within landing pad (ends an episode)\n",
    "- +10 for each leg currently on the ground (lifting a leg incurs a -10 reward)\n",
    "- -0.3 for each frame the main engine is used\n",
    "- -0.03 for using the side engines\n",
    "- There are miscellaneous positive (negative) rewards for decreasing (increasing) the distance to the landing pads.\n",
    "\n",
    "The rewards incentivise the agent for landing inside the landing pad on both legs, while using the least amount of fuel as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyglet\n",
    "# !pip install \"gym[Box_2D]\"\n",
    "# !pip install tensorflow\n",
    "# !pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddailisan/.conda-envs/deeprl/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # this is a CPU-bound process\n",
    "seed = 42\n",
    "\n",
    "# load the environment from openai gym\n",
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.seed(seed)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Q-network\n",
    "\n",
    "Our model will be a fully connected neural network with two [64,64] hidden layers that takes in state observations $s$as input.\n",
    "It has four outputs, representing $Q(s, \\mathrm{do nothing})$, \n",
    "$Q(s, \\mathrm{fire left})$, $Q(s, \\mathrm{fire main})$, and $Q(s, \\mathrm{fire right})$. \n",
    "In effect, the network is trying to predict the *expected return* of taking each action given the current input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n # this should be 4\n",
    "num_observations =  len(state) # this is 8\n",
    "\n",
    "def create_q_model():\n",
    "    inputs = layers.Input(shape=(num_observations))\n",
    "\n",
    "    layer1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Dense(64, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Dense(64, activation=\"relu\")(layer2)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=None)(layer3)\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay is a useful trick used in DQNs, particularly when subsequent states are highly correlated to each other.\n",
    "Instead of batching consecutive experiences together and using this to train the DQN, we can instead temporarily store the recent experiences of the agent in a buffer.\n",
    "This allows us to reuse this data later.\n",
    "Random samples from the replay buffer results in a batch of transitions that are decorrelated.\n",
    "It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "The replay buffer is a first-in-first-out (FIFO) storage with finite capacity, which we will implement as a `deque`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append((*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN algorithm\n",
    "\n",
    "\n",
    "Our aim is to train a policy that maximizes the discounted,\n",
    "cumulative reward\n",
    "$R = \\sum_{t=t_0}^{\\tau} \\gamma^{t} r_t$, where\n",
    "$R$ is also known as the *return*. The discount,\n",
    "$\\gamma$, is a constant between $0$ and $1$\n",
    "that ensures the sum converges.\n",
    "The discount is a weight that makes rewards from the uncertain far\n",
    "future less important than the ones in the near future.\n",
    "\n",
    "$Q$-learning tries to find the function\n",
    "$Q(s,a)$ that rstimates our return, if we were to take an action in a given\n",
    "state.\n",
    "This allows us to construct a policy $\\pi$ that maximizes our\n",
    "rewards:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\!\\max_a \\ Q(s, a) $$\n",
    "\n",
    "The challenge here is to find $Q$ that suitably defines our environment.\n",
    "Because neural networks are universal function\n",
    "approximators, one approach is to train a neural network to resemble $Q$.\n",
    "This offers a vast improvement over the tabular approach, which can get numerically intractable once there are a lot more states and actions to consider, as is in a more complex environment.\n",
    "\n",
    "We can use the Bellman Equation:\n",
    "$$ Q(s,a)= \\mathbb{E}(r + \\gamma \\max_{a} Q(s',a)) $$\n",
    "to define a loss function for our problem.\n",
    "Here, we use the temporal difference error, $\\delta$:\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "as the loss function.\n",
    "In addition to this error, we use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss) to train the neural network.\n",
    "For small errors, the Huber loss behaves similar to the mean squared error, while for large errors it is similar to the mean absolute error.\n",
    "The Huber loss is more robust to outliers due to noisy estimates of $Q$.\n",
    "The network is trained over a batch of transitions $B$ sampled from the replay memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "   \n",
    "For convenience and numerical stability reasons, we also make use of two neural networks: the policy and target networks.\n",
    "The policy network represents the first $Q$ term in the temporal difference error, while the target network is the second $Q$ term.\n",
    "The target network copies its weights from the policy network over a longer interval.\n",
    "Avoiding frequent updates to the target network ensures the stability of training the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 23:25:56.640178: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-02-05 23:25:56.640243: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-02-05 23:25:56.640768: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model_policy = create_q_model()\n",
    "\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every `update_target_network` steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Some hyperparameters:\n",
    "\n",
    "-  `epsilon_max`, `epsilon_min`, and `exploration_fraction` control the annealed value of epsilon over training steps.\n",
    "   This allows us to decay the emount of exploration of the agent over time.\n",
    "-  `update_target_network` sets the interval on how often the target network is updated.\n",
    "-  `train_freq` is the number of actions before the policy network weights are updated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f70044cf710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f70044cf710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "running reward: -151.32 at episode 21, frames: 2000\n",
      "running reward: -99.20 at episode 41, frames: 4000\n",
      "running reward: -78.82 at episode 54, frames: 6000\n",
      "running reward: -69.35 at episode 58, frames: 8000\n",
      "running reward: -58.75 at episode 60, frames: 10000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "running reward: -67.31 at episode 62, frames: 12000\n",
      "running reward: -27.84 at episode 67, frames: 14000\n",
      "running reward: -8.61 at episode 72, frames: 16000\n",
      "running reward: 18.65 at episode 77, frames: 18000\n",
      "running reward: 33.82 at episode 81, frames: 20000\n",
      "running reward: 37.62 at episode 83, frames: 22000\n",
      "running reward: 23.78 at episode 86, frames: 24000\n",
      "running reward: 8.16 at episode 89, frames: 26000\n",
      "running reward: 16.02 at episode 93, frames: 28000\n",
      "running reward: 9.35 at episode 97, frames: 30000\n",
      "running reward: 14.25 at episode 100, frames: 32000\n",
      "running reward: 14.45 at episode 102, frames: 34000\n",
      "running reward: 28.55 at episode 106, frames: 36000\n",
      "running reward: 21.26 at episode 108, frames: 38000\n",
      "running reward: 18.89 at episode 110, frames: 40000\n",
      "running reward: 16.07 at episode 112, frames: 42000\n",
      "running reward: -2.36 at episode 115, frames: 44000\n",
      "running reward: -16.80 at episode 117, frames: 46000\n",
      "running reward: -38.40 at episode 120, frames: 48000\n",
      "running reward: -45.34 at episode 122, frames: 50000\n",
      "running reward: -57.75 at episode 124, frames: 52000\n",
      "running reward: -61.18 at episode 126, frames: 54000\n",
      "running reward: -56.39 at episode 129, frames: 56000\n",
      "running reward: -50.92 at episode 131, frames: 58000\n",
      "running reward: -50.88 at episode 133, frames: 60000\n",
      "running reward: -54.32 at episode 135, frames: 62000\n",
      "running reward: -57.39 at episode 137, frames: 64000\n",
      "running reward: -49.50 at episode 140, frames: 66000\n",
      "running reward: -39.10 at episode 143, frames: 68000\n",
      "running reward: -33.53 at episode 146, frames: 70000\n",
      "running reward: -32.13 at episode 148, frames: 72000\n",
      "running reward: -35.45 at episode 151, frames: 74000\n",
      "running reward: -36.90 at episode 153, frames: 76000\n",
      "running reward: -22.85 at episode 156, frames: 78000\n",
      "running reward: -21.82 at episode 158, frames: 80000\n",
      "running reward: -9.01 at episode 164, frames: 82000\n",
      "running reward: -3.69 at episode 168, frames: 84000\n",
      "running reward: 15.98 at episode 171, frames: 86000\n",
      "running reward: 20.24 at episode 173, frames: 88000\n",
      "running reward: 15.58 at episode 176, frames: 90000\n",
      "running reward: 22.43 at episode 179, frames: 92000\n",
      "running reward: 44.19 at episode 183, frames: 94000\n",
      "running reward: 66.45 at episode 186, frames: 96000\n",
      "running reward: 63.36 at episode 188, frames: 98000\n",
      "running reward: 64.36 at episode 191, frames: 100000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon_min = 0.05  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon = epsilon_max  # Epsilon greedy parameter\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000 # just a safety constraint\n",
    "exploration_fraction = 0.1 # Number of frames for exploration\n",
    "buffer_size = 50000 # Maximum replay length\n",
    "train_freq = 4 # Train the model after 4 actions\n",
    "update_target_network = 500 # How often to update the target network\n",
    "\n",
    "# Deepmind paper used RMSProp however then Adam optimizer is faster\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "episode_reward_history = [0.]\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "\n",
    "loss_function = keras.losses.Huber() # Using huber loss for stability\n",
    "\n",
    "# Experience replay buffers\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "num_timesteps = 100000 # longer to train\n",
    "# num_timesteps = 10000 # debug\n",
    "epsilon_greedy_frames = num_timesteps*exploration_fraction\n",
    "\n",
    "state = env.reset()\n",
    "step_count = 0\n",
    "for frame_count in range(1,num_timesteps+1):\n",
    "    # env.render(); Adding this line would show the attempts\n",
    "    # of the agent in a pop up window.\n",
    "\n",
    "    # Use epsilon-greedy for exploration\n",
    "    if epsilon > np.random.rand(1)[0]:\n",
    "        # Take random action\n",
    "        action = np.random.choice(num_actions)\n",
    "    else:\n",
    "        # Predict action Q-values from state\n",
    "        action_probs = model_policy(state[np.newaxis], training=False)\n",
    "        # Take best action\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    # Linear Decay probability of taking random action\n",
    "    epsilon -= (epsilon_max - epsilon_min)/epsilon_greedy_frames\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    # Apply the sampled action in our environment\n",
    "    state_next, reward, done, _ = env.step(action)\n",
    "\n",
    "#     episode_reward += reward\n",
    "    episode_reward_history[-1] += reward\n",
    "    \n",
    "    # Save actions and states in replay buffer\n",
    "    # replay_buffer.append((action, state, state_next, reward, done))\n",
    "    replay_buffer.push((action, state, state_next, reward, done))\n",
    "\n",
    "    state = state_next\n",
    "\n",
    "    # Update every fourth frame and once batch size is over 32\n",
    "    if frame_count % train_freq == 0 and len(replay_buffer) > batch_size:\n",
    "        # sample the replay buffer\n",
    "        samples = replay_buffer.sample(batch_size)\n",
    "        action_sample = [sample[0] for sample in samples]\n",
    "        state_sample = np.array([sample[1] for sample in samples])\n",
    "        state_next_sample = np.array([sample[2] for sample in samples])\n",
    "        rewards_sample = [sample[3] for sample in samples]\n",
    "        done_sample = tf.convert_to_tensor(\n",
    "            [float(sample[4]) for sample in samples]\n",
    "        )\n",
    "\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = model_target.predict(state_next_sample)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)*(1 - done_sample)\n",
    "        # final frame has no future reward\n",
    "        \n",
    "        # # If final frame set the last value to -1\n",
    "        # updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = model_policy(state_sample)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, model_policy.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model_policy.trainable_variables))\n",
    "\n",
    "    if frame_count % update_target_network == 0:\n",
    "        # update the the target network with new weights\n",
    "        model_target.set_weights(model_policy.get_weights())\n",
    "        # Log details\n",
    "        if frame_count%(update_target_network*4) == 0:\n",
    "            template = f\"running reward: {running_reward:.2f} at episode {episode_count}, frames: {frame_count}\"\n",
    "            print(template)\n",
    "\n",
    "    step_count +=1\n",
    "    if step_count==max_steps_per_episode:\n",
    "        # its taking too long, reset\n",
    "        done = True\n",
    "        step_count = 0\n",
    "        \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "            \n",
    "        # Update running reward to check condition for solving\n",
    "        if len(episode_reward_history) > 20:\n",
    "            del episode_reward_history[:1]\n",
    "        running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "        episode_reward_history.append(0)\n",
    "\n",
    "        episode_count += 1\n",
    "    \n",
    "\n",
    "    if frame_count in [1000, 10000, 100000]:\n",
    "        model_policy.save(f\"dqn_{frame_count}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save this trained model for reuse later (as it takes some time to train the model until it performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -13.67\t Num episodes: 6\n"
     ]
    }
   ],
   "source": [
    "model_policy=keras.models.load_model(f\"dqn_100000.h5\", compile=False)\n",
    "state = env.reset()\n",
    "done = False\n",
    "episode_rewards=[0]\n",
    "steps=0\n",
    "for i in range(5000):  \n",
    "#     env.render() # for visualization, must be done on a local machine\n",
    "\n",
    "    action_probs = model_policy(state[np.newaxis], training=False)\n",
    "    action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    # Apply the sampled action in our environment\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    episode_rewards[-1] += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if steps==max_steps_per_episode:\n",
    "        done = True\n",
    "        steps = 0\n",
    "        \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "\n",
    "# Compute mean reward for the last 100 episodes\n",
    "print(f\"Mean reward: {np.mean(episode_rewards[-100:]):.2f}\\t Num episodes: {len(episode_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "2. https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "3. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading\n",
    "4. https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeprl]",
   "language": "python",
   "name": "conda-env-deeprl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
